{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030132ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:38.586299Z",
     "start_time": "2021-07-09T09:25:35.404415Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaf698ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:38.619128Z",
     "start_time": "2021-07-09T09:25:38.589506Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586c163e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:38.971266Z",
     "start_time": "2021-07-09T09:25:38.623027Z"
    }
   },
   "outputs": [],
   "source": [
    "#char encoding\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# text encoding\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117b4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-08T21:04:58.668538Z",
     "start_time": "2021-06-08T21:04:58.373052Z"
    }
   },
   "source": [
    "### Pre-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffa37ac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:38.978971Z",
     "start_time": "2021-07-09T09:25:38.974736Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Inicializa array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Preenche com valor 1\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Reshape\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d30e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:38.997035Z",
     "start_time": "2021-07-09T09:25:38.989487Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3804f37",
   "metadata": {},
   "source": [
    "### Define a arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea4a97f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:39.009704Z",
     "start_time": "2021-07-09T09:25:39.000551Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #definir lstm input_size, hidden_size, num_layers\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #definir dropout\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #definir camada fc num_hidden input_size\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                \n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(r_output)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Gera tensores de tamanho n_layers x betch_size x n_hidden\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac4863a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T09:25:39.027147Z",
     "start_time": "2021-07-09T09:25:39.015330Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    #dados de treino/validacao\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encoding\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            # Cria vari√°veis para hidden state \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # saida do modelo\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f262a84",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d45111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T10:49:59.886457Z",
     "start_time": "2021-07-09T10:49:59.867489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (lstm): LSTM(83, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden=256\n",
    "n_layers=2\n",
    "\n",
    "net = CharLSTM(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73833f9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T23:09:47.206004Z",
     "start_time": "2021-07-09T10:50:00.714797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/110... Step: 10... Loss: 3.3648... Val Loss: 3.2459\n",
      "Epoch: 1/110... Step: 20... Loss: 3.1842... Val Loss: 3.1352\n",
      "Epoch: 1/110... Step: 30... Loss: 3.1689... Val Loss: 3.1292\n",
      "Epoch: 1/110... Step: 40... Loss: 3.1384... Val Loss: 3.1213\n",
      "Epoch: 1/110... Step: 50... Loss: 3.1609... Val Loss: 3.1198\n",
      "Epoch: 1/110... Step: 60... Loss: 3.1320... Val Loss: 3.1184\n",
      "Epoch: 1/110... Step: 70... Loss: 3.1185... Val Loss: 3.1174\n",
      "Epoch: 1/110... Step: 80... Loss: 3.1399... Val Loss: 3.1164\n",
      "Epoch: 1/110... Step: 90... Loss: 3.1361... Val Loss: 3.1147\n",
      "Epoch: 1/110... Step: 100... Loss: 3.1273... Val Loss: 3.1122\n",
      "Epoch: 1/110... Step: 110... Loss: 3.1266... Val Loss: 3.1079\n",
      "Epoch: 1/110... Step: 120... Loss: 3.1002... Val Loss: 3.1008\n",
      "Epoch: 1/110... Step: 130... Loss: 3.1063... Val Loss: 3.0872\n",
      "Epoch: 2/110... Step: 140... Loss: 3.0894... Val Loss: 3.0592\n",
      "Epoch: 2/110... Step: 150... Loss: 3.0466... Val Loss: 3.0102\n",
      "Epoch: 2/110... Step: 160... Loss: 2.9986... Val Loss: 2.9374\n",
      "Epoch: 2/110... Step: 170... Loss: 2.8920... Val Loss: 2.8730\n",
      "Epoch: 2/110... Step: 180... Loss: 2.8340... Val Loss: 2.7969\n",
      "Epoch: 2/110... Step: 190... Loss: 2.7625... Val Loss: 2.7322\n",
      "Epoch: 2/110... Step: 200... Loss: 2.7132... Val Loss: 2.6652\n",
      "Epoch: 2/110... Step: 210... Loss: 2.6709... Val Loss: 2.6159\n",
      "Epoch: 2/110... Step: 220... Loss: 2.6290... Val Loss: 2.5779\n",
      "Epoch: 2/110... Step: 230... Loss: 2.5993... Val Loss: 2.5441\n",
      "Epoch: 2/110... Step: 240... Loss: 2.5843... Val Loss: 2.5214\n",
      "Epoch: 2/110... Step: 250... Loss: 2.5253... Val Loss: 2.4939\n",
      "Epoch: 2/110... Step: 260... Loss: 2.5136... Val Loss: 2.4731\n",
      "Epoch: 2/110... Step: 270... Loss: 2.4992... Val Loss: 2.4522\n",
      "Epoch: 3/110... Step: 280... Loss: 2.4913... Val Loss: 2.4307\n",
      "Epoch: 3/110... Step: 290... Loss: 2.4704... Val Loss: 2.4103\n",
      "Epoch: 3/110... Step: 300... Loss: 2.4551... Val Loss: 2.3952\n",
      "Epoch: 3/110... Step: 310... Loss: 2.4422... Val Loss: 2.3806\n",
      "Epoch: 3/110... Step: 320... Loss: 2.4211... Val Loss: 2.3636\n",
      "Epoch: 3/110... Step: 330... Loss: 2.3910... Val Loss: 2.3504\n",
      "Epoch: 3/110... Step: 340... Loss: 2.3924... Val Loss: 2.3394\n",
      "Epoch: 3/110... Step: 350... Loss: 2.4033... Val Loss: 2.3231\n",
      "Epoch: 3/110... Step: 360... Loss: 2.3354... Val Loss: 2.3116\n",
      "Epoch: 3/110... Step: 370... Loss: 2.3601... Val Loss: 2.3012\n",
      "Epoch: 3/110... Step: 380... Loss: 2.3356... Val Loss: 2.2882\n",
      "Epoch: 3/110... Step: 390... Loss: 2.3217... Val Loss: 2.2733\n",
      "Epoch: 3/110... Step: 400... Loss: 2.2982... Val Loss: 2.2597\n",
      "Epoch: 3/110... Step: 410... Loss: 2.3084... Val Loss: 2.2486\n",
      "Epoch: 4/110... Step: 420... Loss: 2.2833... Val Loss: 2.2366\n",
      "Epoch: 4/110... Step: 430... Loss: 2.2763... Val Loss: 2.2248\n",
      "Epoch: 4/110... Step: 440... Loss: 2.2732... Val Loss: 2.2173\n",
      "Epoch: 4/110... Step: 450... Loss: 2.2239... Val Loss: 2.2064\n",
      "Epoch: 4/110... Step: 460... Loss: 2.2374... Val Loss: 2.1972\n",
      "Epoch: 4/110... Step: 470... Loss: 2.2386... Val Loss: 2.1892\n",
      "Epoch: 4/110... Step: 480... Loss: 2.2172... Val Loss: 2.1765\n",
      "Epoch: 4/110... Step: 490... Loss: 2.2251... Val Loss: 2.1664\n",
      "Epoch: 4/110... Step: 500... Loss: 2.2252... Val Loss: 2.1572\n",
      "Epoch: 4/110... Step: 510... Loss: 2.2232... Val Loss: 2.1516\n",
      "Epoch: 4/110... Step: 520... Loss: 2.2236... Val Loss: 2.1434\n",
      "Epoch: 4/110... Step: 530... Loss: 2.1784... Val Loss: 2.1276\n",
      "Epoch: 4/110... Step: 540... Loss: 2.1487... Val Loss: 2.1206\n",
      "Epoch: 4/110... Step: 550... Loss: 2.1798... Val Loss: 2.1137\n",
      "Epoch: 5/110... Step: 560... Loss: 2.1537... Val Loss: 2.1076\n",
      "Epoch: 5/110... Step: 570... Loss: 2.1412... Val Loss: 2.1005\n",
      "Epoch: 5/110... Step: 580... Loss: 2.1250... Val Loss: 2.0941\n",
      "Epoch: 5/110... Step: 590... Loss: 2.1400... Val Loss: 2.0820\n",
      "Epoch: 5/110... Step: 600... Loss: 2.1142... Val Loss: 2.0721\n",
      "Epoch: 5/110... Step: 610... Loss: 2.1235... Val Loss: 2.0683\n",
      "Epoch: 5/110... Step: 620... Loss: 2.0983... Val Loss: 2.0695\n",
      "Epoch: 5/110... Step: 630... Loss: 2.1407... Val Loss: 2.0501\n",
      "Epoch: 5/110... Step: 640... Loss: 2.1076... Val Loss: 2.0428\n",
      "Epoch: 5/110... Step: 650... Loss: 2.0913... Val Loss: 2.0365\n",
      "Epoch: 5/110... Step: 660... Loss: 2.0539... Val Loss: 2.0286\n",
      "Epoch: 5/110... Step: 670... Loss: 2.0950... Val Loss: 2.0217\n",
      "Epoch: 5/110... Step: 680... Loss: 2.0912... Val Loss: 2.0140\n",
      "Epoch: 5/110... Step: 690... Loss: 2.0490... Val Loss: 2.0106\n",
      "Epoch: 6/110... Step: 700... Loss: 2.0478... Val Loss: 1.9999\n",
      "Epoch: 6/110... Step: 710... Loss: 2.0475... Val Loss: 1.9943\n",
      "Epoch: 6/110... Step: 720... Loss: 2.0371... Val Loss: 1.9896\n",
      "Epoch: 6/110... Step: 730... Loss: 2.0477... Val Loss: 1.9836\n",
      "Epoch: 6/110... Step: 740... Loss: 2.0249... Val Loss: 1.9753\n",
      "Epoch: 6/110... Step: 750... Loss: 1.9876... Val Loss: 1.9706\n",
      "Epoch: 6/110... Step: 760... Loss: 2.0310... Val Loss: 1.9643\n",
      "Epoch: 6/110... Step: 770... Loss: 2.0217... Val Loss: 1.9586\n",
      "Epoch: 6/110... Step: 780... Loss: 1.9953... Val Loss: 1.9522\n",
      "Epoch: 6/110... Step: 790... Loss: 1.9924... Val Loss: 1.9446\n",
      "Epoch: 6/110... Step: 800... Loss: 1.9884... Val Loss: 1.9439\n",
      "Epoch: 6/110... Step: 810... Loss: 1.9933... Val Loss: 1.9348\n",
      "Epoch: 6/110... Step: 820... Loss: 1.9758... Val Loss: 1.9316\n",
      "Epoch: 6/110... Step: 830... Loss: 2.0003... Val Loss: 1.9273\n",
      "Epoch: 7/110... Step: 840... Loss: 1.9543... Val Loss: 1.9149\n",
      "Epoch: 7/110... Step: 850... Loss: 1.9633... Val Loss: 1.9101\n",
      "Epoch: 7/110... Step: 860... Loss: 1.9568... Val Loss: 1.9050\n",
      "Epoch: 7/110... Step: 870... Loss: 1.9575... Val Loss: 1.8982\n",
      "Epoch: 7/110... Step: 880... Loss: 1.9562... Val Loss: 1.8916\n",
      "Epoch: 7/110... Step: 890... Loss: 1.9500... Val Loss: 1.8906\n",
      "Epoch: 7/110... Step: 900... Loss: 1.9281... Val Loss: 1.8833\n",
      "Epoch: 7/110... Step: 910... Loss: 1.9155... Val Loss: 1.8784\n",
      "Epoch: 7/110... Step: 920... Loss: 1.9330... Val Loss: 1.8728\n",
      "Epoch: 7/110... Step: 930... Loss: 1.9164... Val Loss: 1.8682\n",
      "Epoch: 7/110... Step: 940... Loss: 1.9091... Val Loss: 1.8610\n",
      "Epoch: 7/110... Step: 950... Loss: 1.9240... Val Loss: 1.8581\n",
      "Epoch: 7/110... Step: 960... Loss: 1.9302... Val Loss: 1.8557\n",
      "Epoch: 7/110... Step: 970... Loss: 1.9319... Val Loss: 1.8515\n",
      "Epoch: 8/110... Step: 980... Loss: 1.9017... Val Loss: 1.8421\n",
      "Epoch: 8/110... Step: 990... Loss: 1.8968... Val Loss: 1.8402\n",
      "Epoch: 8/110... Step: 1000... Loss: 1.8845... Val Loss: 1.8323\n",
      "Epoch: 8/110... Step: 1010... Loss: 1.9202... Val Loss: 1.8287\n",
      "Epoch: 8/110... Step: 1020... Loss: 1.8868... Val Loss: 1.8219\n",
      "Epoch: 8/110... Step: 1030... Loss: 1.8699... Val Loss: 1.8177\n",
      "Epoch: 8/110... Step: 1040... Loss: 1.8711... Val Loss: 1.8175\n",
      "Epoch: 8/110... Step: 1050... Loss: 1.8749... Val Loss: 1.8099\n",
      "Epoch: 8/110... Step: 1060... Loss: 1.8726... Val Loss: 1.8051\n",
      "Epoch: 8/110... Step: 1070... Loss: 1.8755... Val Loss: 1.8017\n",
      "Epoch: 8/110... Step: 1080... Loss: 1.8669... Val Loss: 1.7984\n",
      "Epoch: 8/110... Step: 1090... Loss: 1.8500... Val Loss: 1.7951\n",
      "Epoch: 8/110... Step: 1100... Loss: 1.8394... Val Loss: 1.7894\n",
      "Epoch: 8/110... Step: 1110... Loss: 1.8391... Val Loss: 1.7872\n",
      "Epoch: 9/110... Step: 1120... Loss: 1.8603... Val Loss: 1.7823\n",
      "Epoch: 9/110... Step: 1130... Loss: 1.8409... Val Loss: 1.7807\n",
      "Epoch: 9/110... Step: 1140... Loss: 1.8503... Val Loss: 1.7712\n",
      "Epoch: 9/110... Step: 1150... Loss: 1.8588... Val Loss: 1.7702\n",
      "Epoch: 9/110... Step: 1160... Loss: 1.8079... Val Loss: 1.7651\n",
      "Epoch: 9/110... Step: 1170... Loss: 1.8093... Val Loss: 1.7611\n",
      "Epoch: 9/110... Step: 1180... Loss: 1.8147... Val Loss: 1.7624\n",
      "Epoch: 9/110... Step: 1190... Loss: 1.8438... Val Loss: 1.7567\n",
      "Epoch: 9/110... Step: 1200... Loss: 1.8014... Val Loss: 1.7521\n",
      "Epoch: 9/110... Step: 1210... Loss: 1.8002... Val Loss: 1.7484\n",
      "Epoch: 9/110... Step: 1220... Loss: 1.7985... Val Loss: 1.7440\n",
      "Epoch: 9/110... Step: 1230... Loss: 1.7868... Val Loss: 1.7429\n",
      "Epoch: 9/110... Step: 1240... Loss: 1.7839... Val Loss: 1.7355\n",
      "Epoch: 9/110... Step: 1250... Loss: 1.7924... Val Loss: 1.7380\n",
      "Epoch: 10/110... Step: 1260... Loss: 1.7942... Val Loss: 1.7295\n",
      "Epoch: 10/110... Step: 1270... Loss: 1.7766... Val Loss: 1.7271\n",
      "Epoch: 10/110... Step: 1280... Loss: 1.8057... Val Loss: 1.7220\n",
      "Epoch: 10/110... Step: 1290... Loss: 1.7833... Val Loss: 1.7201\n",
      "Epoch: 10/110... Step: 1300... Loss: 1.7653... Val Loss: 1.7178\n",
      "Epoch: 10/110... Step: 1310... Loss: 1.7862... Val Loss: 1.7152\n",
      "Epoch: 10/110... Step: 1320... Loss: 1.7620... Val Loss: 1.7150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/110... Step: 1330... Loss: 1.7670... Val Loss: 1.7113\n",
      "Epoch: 10/110... Step: 1340... Loss: 1.7493... Val Loss: 1.7086\n",
      "Epoch: 10/110... Step: 1350... Loss: 1.7440... Val Loss: 1.7033\n",
      "Epoch: 10/110... Step: 1360... Loss: 1.7448... Val Loss: 1.7010\n",
      "Epoch: 10/110... Step: 1370... Loss: 1.7368... Val Loss: 1.6981\n",
      "Epoch: 10/110... Step: 1380... Loss: 1.7642... Val Loss: 1.6912\n",
      "Epoch: 10/110... Step: 1390... Loss: 1.7695... Val Loss: 1.6968\n",
      "Epoch: 11/110... Step: 1400... Loss: 1.7695... Val Loss: 1.6853\n",
      "Epoch: 11/110... Step: 1410... Loss: 1.7752... Val Loss: 1.6816\n",
      "Epoch: 11/110... Step: 1420... Loss: 1.7612... Val Loss: 1.6802\n",
      "Epoch: 11/110... Step: 1430... Loss: 1.7309... Val Loss: 1.6767\n",
      "Epoch: 11/110... Step: 1440... Loss: 1.7735... Val Loss: 1.6746\n",
      "Epoch: 11/110... Step: 1450... Loss: 1.7031... Val Loss: 1.6731\n",
      "Epoch: 11/110... Step: 1460... Loss: 1.7163... Val Loss: 1.6719\n",
      "Epoch: 11/110... Step: 1470... Loss: 1.7176... Val Loss: 1.6677\n",
      "Epoch: 11/110... Step: 1480... Loss: 1.7342... Val Loss: 1.6654\n",
      "Epoch: 11/110... Step: 1490... Loss: 1.7189... Val Loss: 1.6643\n",
      "Epoch: 11/110... Step: 1500... Loss: 1.7068... Val Loss: 1.6634\n",
      "Epoch: 11/110... Step: 1510... Loss: 1.6804... Val Loss: 1.6593\n",
      "Epoch: 11/110... Step: 1520... Loss: 1.7352... Val Loss: 1.6533\n",
      "Epoch: 12/110... Step: 1530... Loss: 1.7503... Val Loss: 1.6562\n",
      "Epoch: 12/110... Step: 1540... Loss: 1.7358... Val Loss: 1.6480\n",
      "Epoch: 12/110... Step: 1550... Loss: 1.7341... Val Loss: 1.6451\n",
      "Epoch: 12/110... Step: 1560... Loss: 1.7338... Val Loss: 1.6421\n",
      "Epoch: 12/110... Step: 1570... Loss: 1.6920... Val Loss: 1.6411\n",
      "Epoch: 12/110... Step: 1580... Loss: 1.6640... Val Loss: 1.6385\n",
      "Epoch: 12/110... Step: 1590... Loss: 1.6727... Val Loss: 1.6396\n",
      "Epoch: 12/110... Step: 1600... Loss: 1.6980... Val Loss: 1.6394\n",
      "Epoch: 12/110... Step: 1610... Loss: 1.6788... Val Loss: 1.6395\n",
      "Epoch: 12/110... Step: 1620... Loss: 1.7004... Val Loss: 1.6337\n",
      "Epoch: 12/110... Step: 1630... Loss: 1.7161... Val Loss: 1.6286\n",
      "Epoch: 12/110... Step: 1640... Loss: 1.6667... Val Loss: 1.6318\n",
      "Epoch: 12/110... Step: 1650... Loss: 1.6459... Val Loss: 1.6268\n",
      "Epoch: 12/110... Step: 1660... Loss: 1.6942... Val Loss: 1.6219\n",
      "Epoch: 13/110... Step: 1670... Loss: 1.6877... Val Loss: 1.6210\n",
      "Epoch: 13/110... Step: 1680... Loss: 1.6814... Val Loss: 1.6167\n",
      "Epoch: 13/110... Step: 1690... Loss: 1.6563... Val Loss: 1.6125\n",
      "Epoch: 13/110... Step: 1700... Loss: 1.6694... Val Loss: 1.6095\n",
      "Epoch: 13/110... Step: 1710... Loss: 1.6448... Val Loss: 1.6083\n",
      "Epoch: 13/110... Step: 1720... Loss: 1.6599... Val Loss: 1.6080\n",
      "Epoch: 13/110... Step: 1730... Loss: 1.6851... Val Loss: 1.6030\n",
      "Epoch: 13/110... Step: 1740... Loss: 1.6554... Val Loss: 1.6031\n",
      "Epoch: 13/110... Step: 1750... Loss: 1.6248... Val Loss: 1.6081\n",
      "Epoch: 13/110... Step: 1760... Loss: 1.6545... Val Loss: 1.5992\n",
      "Epoch: 13/110... Step: 1770... Loss: 1.6608... Val Loss: 1.6019\n",
      "Epoch: 13/110... Step: 1780... Loss: 1.6411... Val Loss: 1.6017\n",
      "Epoch: 13/110... Step: 1790... Loss: 1.6300... Val Loss: 1.5932\n",
      "Epoch: 13/110... Step: 1800... Loss: 1.6462... Val Loss: 1.5949\n",
      "Epoch: 14/110... Step: 1810... Loss: 1.6498... Val Loss: 1.5949\n",
      "Epoch: 14/110... Step: 1820... Loss: 1.6442... Val Loss: 1.5906\n",
      "Epoch: 14/110... Step: 1830... Loss: 1.6507... Val Loss: 1.5827\n",
      "Epoch: 14/110... Step: 1840... Loss: 1.6144... Val Loss: 1.5806\n",
      "Epoch: 14/110... Step: 1850... Loss: 1.5942... Val Loss: 1.5804\n",
      "Epoch: 14/110... Step: 1860... Loss: 1.6491... Val Loss: 1.5785\n",
      "Epoch: 14/110... Step: 1870... Loss: 1.6453... Val Loss: 1.5738\n",
      "Epoch: 14/110... Step: 1880... Loss: 1.6497... Val Loss: 1.5758\n",
      "Epoch: 14/110... Step: 1890... Loss: 1.6551... Val Loss: 1.5762\n",
      "Epoch: 14/110... Step: 1900... Loss: 1.6242... Val Loss: 1.5720\n",
      "Epoch: 14/110... Step: 1910... Loss: 1.6454... Val Loss: 1.5725\n",
      "Epoch: 14/110... Step: 1920... Loss: 1.6196... Val Loss: 1.5754\n",
      "Epoch: 14/110... Step: 1930... Loss: 1.5928... Val Loss: 1.5652\n",
      "Epoch: 14/110... Step: 1940... Loss: 1.6500... Val Loss: 1.5644\n",
      "Epoch: 15/110... Step: 1950... Loss: 1.6222... Val Loss: 1.5694\n",
      "Epoch: 15/110... Step: 1960... Loss: 1.6114... Val Loss: 1.5631\n",
      "Epoch: 15/110... Step: 1970... Loss: 1.6038... Val Loss: 1.5570\n",
      "Epoch: 15/110... Step: 1980... Loss: 1.6014... Val Loss: 1.5571\n",
      "Epoch: 15/110... Step: 1990... Loss: 1.5944... Val Loss: 1.5578\n",
      "Epoch: 15/110... Step: 2000... Loss: 1.5840... Val Loss: 1.5548\n",
      "Epoch: 15/110... Step: 2010... Loss: 1.5930... Val Loss: 1.5524\n",
      "Epoch: 15/110... Step: 2020... Loss: 1.6187... Val Loss: 1.5496\n",
      "Epoch: 15/110... Step: 2030... Loss: 1.6018... Val Loss: 1.5512\n",
      "Epoch: 15/110... Step: 2040... Loss: 1.5912... Val Loss: 1.5476\n",
      "Epoch: 15/110... Step: 2050... Loss: 1.5721... Val Loss: 1.5457\n",
      "Epoch: 15/110... Step: 2060... Loss: 1.6048... Val Loss: 1.5524\n",
      "Epoch: 15/110... Step: 2070... Loss: 1.6015... Val Loss: 1.5421\n",
      "Epoch: 15/110... Step: 2080... Loss: 1.5882... Val Loss: 1.5394\n",
      "Epoch: 16/110... Step: 2090... Loss: 1.5907... Val Loss: 1.5459\n",
      "Epoch: 16/110... Step: 2100... Loss: 1.5963... Val Loss: 1.5403\n",
      "Epoch: 16/110... Step: 2110... Loss: 1.5787... Val Loss: 1.5365\n",
      "Epoch: 16/110... Step: 2120... Loss: 1.5844... Val Loss: 1.5318\n",
      "Epoch: 16/110... Step: 2130... Loss: 1.5675... Val Loss: 1.5388\n",
      "Epoch: 16/110... Step: 2140... Loss: 1.5560... Val Loss: 1.5338\n",
      "Epoch: 16/110... Step: 2150... Loss: 1.6006... Val Loss: 1.5337\n",
      "Epoch: 16/110... Step: 2160... Loss: 1.5834... Val Loss: 1.5333\n",
      "Epoch: 16/110... Step: 2170... Loss: 1.5637... Val Loss: 1.5322\n",
      "Epoch: 16/110... Step: 2180... Loss: 1.5623... Val Loss: 1.5253\n",
      "Epoch: 16/110... Step: 2190... Loss: 1.5759... Val Loss: 1.5265\n",
      "Epoch: 16/110... Step: 2200... Loss: 1.5656... Val Loss: 1.5293\n",
      "Epoch: 16/110... Step: 2210... Loss: 1.5308... Val Loss: 1.5237\n",
      "Epoch: 16/110... Step: 2220... Loss: 1.5903... Val Loss: 1.5277\n",
      "Epoch: 17/110... Step: 2230... Loss: 1.5489... Val Loss: 1.5301\n",
      "Epoch: 17/110... Step: 2240... Loss: 1.5583... Val Loss: 1.5236\n",
      "Epoch: 17/110... Step: 2250... Loss: 1.5474... Val Loss: 1.5231\n",
      "Epoch: 17/110... Step: 2260... Loss: 1.5587... Val Loss: 1.5156\n",
      "Epoch: 17/110... Step: 2270... Loss: 1.5652... Val Loss: 1.5162\n",
      "Epoch: 17/110... Step: 2280... Loss: 1.5494... Val Loss: 1.5136\n",
      "Epoch: 17/110... Step: 2290... Loss: 1.5574... Val Loss: 1.5181\n",
      "Epoch: 17/110... Step: 2300... Loss: 1.5233... Val Loss: 1.5150\n",
      "Epoch: 17/110... Step: 2310... Loss: 1.5497... Val Loss: 1.5120\n",
      "Epoch: 17/110... Step: 2320... Loss: 1.5412... Val Loss: 1.5125\n",
      "Epoch: 17/110... Step: 2330... Loss: 1.5460... Val Loss: 1.5098\n",
      "Epoch: 17/110... Step: 2340... Loss: 1.5587... Val Loss: 1.5162\n",
      "Epoch: 17/110... Step: 2350... Loss: 1.5560... Val Loss: 1.5087\n",
      "Epoch: 17/110... Step: 2360... Loss: 1.5733... Val Loss: 1.5070\n",
      "Epoch: 18/110... Step: 2370... Loss: 1.5469... Val Loss: 1.5104\n",
      "Epoch: 18/110... Step: 2380... Loss: 1.5369... Val Loss: 1.5069\n",
      "Epoch: 18/110... Step: 2390... Loss: 1.5371... Val Loss: 1.5006\n",
      "Epoch: 18/110... Step: 2400... Loss: 1.5787... Val Loss: 1.4985\n",
      "Epoch: 18/110... Step: 2410... Loss: 1.5501... Val Loss: 1.4960\n",
      "Epoch: 18/110... Step: 2420... Loss: 1.5321... Val Loss: 1.4957\n",
      "Epoch: 18/110... Step: 2430... Loss: 1.5435... Val Loss: 1.4970\n",
      "Epoch: 18/110... Step: 2440... Loss: 1.5214... Val Loss: 1.4999\n",
      "Epoch: 18/110... Step: 2450... Loss: 1.5370... Val Loss: 1.4993\n",
      "Epoch: 18/110... Step: 2460... Loss: 1.5373... Val Loss: 1.4963\n",
      "Epoch: 18/110... Step: 2470... Loss: 1.5357... Val Loss: 1.4938\n",
      "Epoch: 18/110... Step: 2480... Loss: 1.5310... Val Loss: 1.4951\n",
      "Epoch: 18/110... Step: 2490... Loss: 1.5170... Val Loss: 1.4949\n",
      "Epoch: 18/110... Step: 2500... Loss: 1.5315... Val Loss: 1.4916\n",
      "Epoch: 19/110... Step: 2510... Loss: 1.5411... Val Loss: 1.4965\n",
      "Epoch: 19/110... Step: 2520... Loss: 1.5372... Val Loss: 1.4891\n",
      "Epoch: 19/110... Step: 2530... Loss: 1.5401... Val Loss: 1.4874\n",
      "Epoch: 19/110... Step: 2540... Loss: 1.5575... Val Loss: 1.4829\n",
      "Epoch: 19/110... Step: 2550... Loss: 1.5002... Val Loss: 1.4837\n",
      "Epoch: 19/110... Step: 2560... Loss: 1.5181... Val Loss: 1.4864\n",
      "Epoch: 19/110... Step: 2570... Loss: 1.5178... Val Loss: 1.4867\n",
      "Epoch: 19/110... Step: 2580... Loss: 1.5524... Val Loss: 1.4844\n",
      "Epoch: 19/110... Step: 2590... Loss: 1.4945... Val Loss: 1.4849\n",
      "Epoch: 19/110... Step: 2600... Loss: 1.5117... Val Loss: 1.4836\n",
      "Epoch: 19/110... Step: 2610... Loss: 1.5079... Val Loss: 1.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/110... Step: 2620... Loss: 1.5026... Val Loss: 1.4835\n",
      "Epoch: 19/110... Step: 2630... Loss: 1.5015... Val Loss: 1.4783\n",
      "Epoch: 19/110... Step: 2640... Loss: 1.5061... Val Loss: 1.4809\n",
      "Epoch: 20/110... Step: 2650... Loss: 1.5231... Val Loss: 1.4800\n",
      "Epoch: 20/110... Step: 2660... Loss: 1.5157... Val Loss: 1.4740\n",
      "Epoch: 20/110... Step: 2670... Loss: 1.5249... Val Loss: 1.4678\n",
      "Epoch: 20/110... Step: 2680... Loss: 1.5065... Val Loss: 1.4745\n",
      "Epoch: 20/110... Step: 2690... Loss: 1.4980... Val Loss: 1.4712\n",
      "Epoch: 20/110... Step: 2700... Loss: 1.5115... Val Loss: 1.4694\n",
      "Epoch: 20/110... Step: 2710... Loss: 1.4892... Val Loss: 1.4680\n",
      "Epoch: 20/110... Step: 2720... Loss: 1.4919... Val Loss: 1.4679\n",
      "Epoch: 20/110... Step: 2730... Loss: 1.4784... Val Loss: 1.4663\n",
      "Epoch: 20/110... Step: 2740... Loss: 1.4879... Val Loss: 1.4685\n",
      "Epoch: 20/110... Step: 2750... Loss: 1.4834... Val Loss: 1.4697\n",
      "Epoch: 20/110... Step: 2760... Loss: 1.4758... Val Loss: 1.4671\n",
      "Epoch: 20/110... Step: 2770... Loss: 1.5097... Val Loss: 1.4659\n",
      "Epoch: 20/110... Step: 2780... Loss: 1.5165... Val Loss: 1.4673\n",
      "Epoch: 21/110... Step: 2790... Loss: 1.5288... Val Loss: 1.4653\n",
      "Epoch: 21/110... Step: 2800... Loss: 1.5320... Val Loss: 1.4646\n",
      "Epoch: 21/110... Step: 2810... Loss: 1.5273... Val Loss: 1.4578\n",
      "Epoch: 21/110... Step: 2820... Loss: 1.4998... Val Loss: 1.4588\n",
      "Epoch: 21/110... Step: 2830... Loss: 1.5277... Val Loss: 1.4555\n",
      "Epoch: 21/110... Step: 2840... Loss: 1.4522... Val Loss: 1.4578\n",
      "Epoch: 21/110... Step: 2850... Loss: 1.4859... Val Loss: 1.4546\n",
      "Epoch: 21/110... Step: 2860... Loss: 1.4799... Val Loss: 1.4545\n",
      "Epoch: 21/110... Step: 2870... Loss: 1.4828... Val Loss: 1.4554\n",
      "Epoch: 21/110... Step: 2880... Loss: 1.4885... Val Loss: 1.4577\n",
      "Epoch: 21/110... Step: 2890... Loss: 1.4626... Val Loss: 1.4524\n",
      "Epoch: 21/110... Step: 2900... Loss: 1.4476... Val Loss: 1.4524\n",
      "Epoch: 21/110... Step: 2910... Loss: 1.4943... Val Loss: 1.4532\n",
      "Epoch: 22/110... Step: 2920... Loss: 1.5424... Val Loss: 1.4581\n",
      "Epoch: 22/110... Step: 2930... Loss: 1.4994... Val Loss: 1.4529\n",
      "Epoch: 22/110... Step: 2940... Loss: 1.5180... Val Loss: 1.4509\n",
      "Epoch: 22/110... Step: 2950... Loss: 1.5125... Val Loss: 1.4489\n",
      "Epoch: 22/110... Step: 2960... Loss: 1.4683... Val Loss: 1.4474\n",
      "Epoch: 22/110... Step: 2970... Loss: 1.4393... Val Loss: 1.4484\n",
      "Epoch: 22/110... Step: 2980... Loss: 1.4528... Val Loss: 1.4472\n",
      "Epoch: 22/110... Step: 2990... Loss: 1.4646... Val Loss: 1.4484\n",
      "Epoch: 22/110... Step: 3000... Loss: 1.4672... Val Loss: 1.4467\n",
      "Epoch: 22/110... Step: 3010... Loss: 1.4667... Val Loss: 1.4425\n",
      "Epoch: 22/110... Step: 3020... Loss: 1.4946... Val Loss: 1.4442\n",
      "Epoch: 22/110... Step: 3030... Loss: 1.4574... Val Loss: 1.4450\n",
      "Epoch: 22/110... Step: 3040... Loss: 1.4379... Val Loss: 1.4450\n",
      "Epoch: 22/110... Step: 3050... Loss: 1.4832... Val Loss: 1.4422\n",
      "Epoch: 23/110... Step: 3060... Loss: 1.4675... Val Loss: 1.4513\n",
      "Epoch: 23/110... Step: 3070... Loss: 1.4694... Val Loss: 1.4421\n",
      "Epoch: 23/110... Step: 3080... Loss: 1.4506... Val Loss: 1.4388\n",
      "Epoch: 23/110... Step: 3090... Loss: 1.4617... Val Loss: 1.4352\n",
      "Epoch: 23/110... Step: 3100... Loss: 1.4336... Val Loss: 1.4375\n",
      "Epoch: 23/110... Step: 3110... Loss: 1.4544... Val Loss: 1.4388\n",
      "Epoch: 23/110... Step: 3120... Loss: 1.4998... Val Loss: 1.4331\n",
      "Epoch: 23/110... Step: 3130... Loss: 1.4675... Val Loss: 1.4355\n",
      "Epoch: 23/110... Step: 3140... Loss: 1.4289... Val Loss: 1.4363\n",
      "Epoch: 23/110... Step: 3150... Loss: 1.4574... Val Loss: 1.4345\n",
      "Epoch: 23/110... Step: 3160... Loss: 1.4657... Val Loss: 1.4337\n",
      "Epoch: 23/110... Step: 3170... Loss: 1.4467... Val Loss: 1.4369\n",
      "Epoch: 23/110... Step: 3180... Loss: 1.4405... Val Loss: 1.4347\n",
      "Epoch: 23/110... Step: 3190... Loss: 1.4577... Val Loss: 1.4324\n",
      "Epoch: 24/110... Step: 3200... Loss: 1.4683... Val Loss: 1.4403\n",
      "Epoch: 24/110... Step: 3210... Loss: 1.4530... Val Loss: 1.4310\n",
      "Epoch: 24/110... Step: 3220... Loss: 1.4759... Val Loss: 1.4288\n",
      "Epoch: 24/110... Step: 3230... Loss: 1.4170... Val Loss: 1.4289\n",
      "Epoch: 24/110... Step: 3240... Loss: 1.4045... Val Loss: 1.4256\n",
      "Epoch: 24/110... Step: 3250... Loss: 1.4555... Val Loss: 1.4293\n",
      "Epoch: 24/110... Step: 3260... Loss: 1.4573... Val Loss: 1.4263\n",
      "Epoch: 24/110... Step: 3270... Loss: 1.4600... Val Loss: 1.4289\n",
      "Epoch: 24/110... Step: 3280... Loss: 1.4779... Val Loss: 1.4264\n",
      "Epoch: 24/110... Step: 3290... Loss: 1.4518... Val Loss: 1.4255\n",
      "Epoch: 24/110... Step: 3300... Loss: 1.4647... Val Loss: 1.4268\n",
      "Epoch: 24/110... Step: 3310... Loss: 1.4490... Val Loss: 1.4264\n",
      "Epoch: 24/110... Step: 3320... Loss: 1.4065... Val Loss: 1.4230\n",
      "Epoch: 24/110... Step: 3330... Loss: 1.4759... Val Loss: 1.4230\n",
      "Epoch: 25/110... Step: 3340... Loss: 1.4537... Val Loss: 1.4284\n",
      "Epoch: 25/110... Step: 3350... Loss: 1.4376... Val Loss: 1.4193\n",
      "Epoch: 25/110... Step: 3360... Loss: 1.4372... Val Loss: 1.4193\n",
      "Epoch: 25/110... Step: 3370... Loss: 1.4318... Val Loss: 1.4182\n",
      "Epoch: 25/110... Step: 3380... Loss: 1.4390... Val Loss: 1.4165\n",
      "Epoch: 25/110... Step: 3390... Loss: 1.4083... Val Loss: 1.4213\n",
      "Epoch: 25/110... Step: 3400... Loss: 1.4334... Val Loss: 1.4181\n",
      "Epoch: 25/110... Step: 3410... Loss: 1.4566... Val Loss: 1.4178\n",
      "Epoch: 25/110... Step: 3420... Loss: 1.4222... Val Loss: 1.4184\n",
      "Epoch: 25/110... Step: 3430... Loss: 1.4400... Val Loss: 1.4140\n",
      "Epoch: 25/110... Step: 3440... Loss: 1.4204... Val Loss: 1.4182\n",
      "Epoch: 25/110... Step: 3450... Loss: 1.4437... Val Loss: 1.4139\n",
      "Epoch: 25/110... Step: 3460... Loss: 1.4380... Val Loss: 1.4158\n",
      "Epoch: 25/110... Step: 3470... Loss: 1.4420... Val Loss: 1.4149\n",
      "Epoch: 26/110... Step: 3480... Loss: 1.4430... Val Loss: 1.4258\n",
      "Epoch: 26/110... Step: 3490... Loss: 1.4298... Val Loss: 1.4112\n",
      "Epoch: 26/110... Step: 3500... Loss: 1.4237... Val Loss: 1.4105\n",
      "Epoch: 26/110... Step: 3510... Loss: 1.4411... Val Loss: 1.4105\n",
      "Epoch: 26/110... Step: 3520... Loss: 1.4125... Val Loss: 1.4096\n",
      "Epoch: 26/110... Step: 3530... Loss: 1.4129... Val Loss: 1.4113\n",
      "Epoch: 26/110... Step: 3540... Loss: 1.4500... Val Loss: 1.4096\n",
      "Epoch: 26/110... Step: 3550... Loss: 1.4279... Val Loss: 1.4099\n",
      "Epoch: 26/110... Step: 3560... Loss: 1.4280... Val Loss: 1.4112\n",
      "Epoch: 26/110... Step: 3570... Loss: 1.4227... Val Loss: 1.4113\n",
      "Epoch: 26/110... Step: 3580... Loss: 1.4315... Val Loss: 1.4103\n",
      "Epoch: 26/110... Step: 3590... Loss: 1.4116... Val Loss: 1.4082\n",
      "Epoch: 26/110... Step: 3600... Loss: 1.3843... Val Loss: 1.4065\n",
      "Epoch: 26/110... Step: 3610... Loss: 1.4339... Val Loss: 1.4064\n",
      "Epoch: 27/110... Step: 3620... Loss: 1.4067... Val Loss: 1.4138\n",
      "Epoch: 27/110... Step: 3630... Loss: 1.4150... Val Loss: 1.4118\n",
      "Epoch: 27/110... Step: 3640... Loss: 1.4120... Val Loss: 1.4040\n",
      "Epoch: 27/110... Step: 3650... Loss: 1.4037... Val Loss: 1.4026\n",
      "Epoch: 27/110... Step: 3660... Loss: 1.4134... Val Loss: 1.4029\n",
      "Epoch: 27/110... Step: 3670... Loss: 1.4275... Val Loss: 1.4017\n",
      "Epoch: 27/110... Step: 3680... Loss: 1.4161... Val Loss: 1.4006\n",
      "Epoch: 27/110... Step: 3690... Loss: 1.3809... Val Loss: 1.4041\n",
      "Epoch: 27/110... Step: 3700... Loss: 1.4150... Val Loss: 1.4021\n",
      "Epoch: 27/110... Step: 3710... Loss: 1.4101... Val Loss: 1.4007\n",
      "Epoch: 27/110... Step: 3720... Loss: 1.4105... Val Loss: 1.3987\n",
      "Epoch: 27/110... Step: 3730... Loss: 1.4273... Val Loss: 1.4024\n",
      "Epoch: 27/110... Step: 3740... Loss: 1.4283... Val Loss: 1.3976\n",
      "Epoch: 27/110... Step: 3750... Loss: 1.4365... Val Loss: 1.4000\n",
      "Epoch: 28/110... Step: 3760... Loss: 1.4032... Val Loss: 1.4082\n",
      "Epoch: 28/110... Step: 3770... Loss: 1.4038... Val Loss: 1.3993\n",
      "Epoch: 28/110... Step: 3780... Loss: 1.4101... Val Loss: 1.3970\n",
      "Epoch: 28/110... Step: 3790... Loss: 1.4363... Val Loss: 1.3952\n",
      "Epoch: 28/110... Step: 3800... Loss: 1.4211... Val Loss: 1.3980\n",
      "Epoch: 28/110... Step: 3810... Loss: 1.4083... Val Loss: 1.3957\n",
      "Epoch: 28/110... Step: 3820... Loss: 1.4205... Val Loss: 1.4023\n",
      "Epoch: 28/110... Step: 3830... Loss: 1.3917... Val Loss: 1.3993\n",
      "Epoch: 28/110... Step: 3840... Loss: 1.4039... Val Loss: 1.4032\n",
      "Epoch: 28/110... Step: 3850... Loss: 1.4063... Val Loss: 1.3978\n",
      "Epoch: 28/110... Step: 3860... Loss: 1.4078... Val Loss: 1.3938\n",
      "Epoch: 28/110... Step: 3870... Loss: 1.4048... Val Loss: 1.3939\n",
      "Epoch: 28/110... Step: 3880... Loss: 1.3947... Val Loss: 1.3928\n",
      "Epoch: 28/110... Step: 3890... Loss: 1.3937... Val Loss: 1.3897\n",
      "Epoch: 29/110... Step: 3900... Loss: 1.4126... Val Loss: 1.4010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29/110... Step: 3910... Loss: 1.4216... Val Loss: 1.3940\n",
      "Epoch: 29/110... Step: 3920... Loss: 1.4256... Val Loss: 1.3890\n",
      "Epoch: 29/110... Step: 3930... Loss: 1.4359... Val Loss: 1.3912\n",
      "Epoch: 29/110... Step: 3940... Loss: 1.4000... Val Loss: 1.3896\n",
      "Epoch: 29/110... Step: 3950... Loss: 1.4033... Val Loss: 1.3906\n",
      "Epoch: 29/110... Step: 3960... Loss: 1.3862... Val Loss: 1.3910\n",
      "Epoch: 29/110... Step: 3970... Loss: 1.4310... Val Loss: 1.3933\n",
      "Epoch: 29/110... Step: 3980... Loss: 1.3832... Val Loss: 1.3932\n",
      "Epoch: 29/110... Step: 3990... Loss: 1.3946... Val Loss: 1.3908\n",
      "Epoch: 29/110... Step: 4000... Loss: 1.3954... Val Loss: 1.3913\n",
      "Epoch: 29/110... Step: 4010... Loss: 1.3821... Val Loss: 1.3934\n",
      "Epoch: 29/110... Step: 4020... Loss: 1.3911... Val Loss: 1.3894\n",
      "Epoch: 29/110... Step: 4030... Loss: 1.3976... Val Loss: 1.3912\n",
      "Epoch: 30/110... Step: 4040... Loss: 1.4126... Val Loss: 1.3897\n",
      "Epoch: 30/110... Step: 4050... Loss: 1.4141... Val Loss: 1.3874\n",
      "Epoch: 30/110... Step: 4060... Loss: 1.4190... Val Loss: 1.3794\n",
      "Epoch: 30/110... Step: 4070... Loss: 1.4013... Val Loss: 1.3831\n",
      "Epoch: 30/110... Step: 4080... Loss: 1.3899... Val Loss: 1.3853\n",
      "Epoch: 30/110... Step: 4090... Loss: 1.4020... Val Loss: 1.3838\n",
      "Epoch: 30/110... Step: 4100... Loss: 1.3835... Val Loss: 1.3843\n",
      "Epoch: 30/110... Step: 4110... Loss: 1.3801... Val Loss: 1.3859\n",
      "Epoch: 30/110... Step: 4120... Loss: 1.3724... Val Loss: 1.3852\n",
      "Epoch: 30/110... Step: 4130... Loss: 1.3643... Val Loss: 1.3844\n",
      "Epoch: 30/110... Step: 4140... Loss: 1.3783... Val Loss: 1.3822\n",
      "Epoch: 30/110... Step: 4150... Loss: 1.3701... Val Loss: 1.3847\n",
      "Epoch: 30/110... Step: 4160... Loss: 1.4084... Val Loss: 1.3794\n",
      "Epoch: 30/110... Step: 4170... Loss: 1.4249... Val Loss: 1.3800\n",
      "Epoch: 31/110... Step: 4180... Loss: 1.4218... Val Loss: 1.3864\n",
      "Epoch: 31/110... Step: 4190... Loss: 1.4285... Val Loss: 1.3819\n",
      "Epoch: 31/110... Step: 4200... Loss: 1.4312... Val Loss: 1.3727\n",
      "Epoch: 31/110... Step: 4210... Loss: 1.3890... Val Loss: 1.3807\n",
      "Epoch: 31/110... Step: 4220... Loss: 1.4060... Val Loss: 1.3778\n",
      "Epoch: 31/110... Step: 4230... Loss: 1.3545... Val Loss: 1.3765\n",
      "Epoch: 31/110... Step: 4240... Loss: 1.3784... Val Loss: 1.3809\n",
      "Epoch: 31/110... Step: 4250... Loss: 1.3772... Val Loss: 1.3846\n",
      "Epoch: 31/110... Step: 4260... Loss: 1.3880... Val Loss: 1.3864\n",
      "Epoch: 31/110... Step: 4270... Loss: 1.3846... Val Loss: 1.3845\n",
      "Epoch: 31/110... Step: 4280... Loss: 1.3677... Val Loss: 1.3828\n",
      "Epoch: 31/110... Step: 4290... Loss: 1.3580... Val Loss: 1.3834\n",
      "Epoch: 31/110... Step: 4300... Loss: 1.3960... Val Loss: 1.3768\n",
      "Epoch: 32/110... Step: 4310... Loss: 1.4454... Val Loss: 1.3802\n",
      "Epoch: 32/110... Step: 4320... Loss: 1.4078... Val Loss: 1.3791\n",
      "Epoch: 32/110... Step: 4330... Loss: 1.4002... Val Loss: 1.3753\n",
      "Epoch: 32/110... Step: 4340... Loss: 1.4130... Val Loss: 1.3728\n",
      "Epoch: 32/110... Step: 4350... Loss: 1.3677... Val Loss: 1.3847\n",
      "Epoch: 32/110... Step: 4360... Loss: 1.3505... Val Loss: 1.3836\n",
      "Epoch: 32/110... Step: 4370... Loss: 1.3460... Val Loss: 1.3801\n",
      "Epoch: 32/110... Step: 4380... Loss: 1.3739... Val Loss: 1.3799\n",
      "Epoch: 32/110... Step: 4390... Loss: 1.3711... Val Loss: 1.3787\n",
      "Epoch: 32/110... Step: 4400... Loss: 1.3713... Val Loss: 1.3774\n",
      "Epoch: 32/110... Step: 4410... Loss: 1.3941... Val Loss: 1.3769\n",
      "Epoch: 32/110... Step: 4420... Loss: 1.3683... Val Loss: 1.3776\n",
      "Epoch: 32/110... Step: 4430... Loss: 1.3493... Val Loss: 1.3824\n",
      "Epoch: 32/110... Step: 4440... Loss: 1.3994... Val Loss: 1.3742\n",
      "Epoch: 33/110... Step: 4450... Loss: 1.3721... Val Loss: 1.3788\n",
      "Epoch: 33/110... Step: 4460... Loss: 1.3865... Val Loss: 1.3752\n",
      "Epoch: 33/110... Step: 4470... Loss: 1.3602... Val Loss: 1.3725\n",
      "Epoch: 33/110... Step: 4480... Loss: 1.3729... Val Loss: 1.3687\n",
      "Epoch: 33/110... Step: 4490... Loss: 1.3360... Val Loss: 1.3776\n",
      "Epoch: 33/110... Step: 4500... Loss: 1.3619... Val Loss: 1.3745\n",
      "Epoch: 33/110... Step: 4510... Loss: 1.4055... Val Loss: 1.3726\n",
      "Epoch: 33/110... Step: 4520... Loss: 1.3721... Val Loss: 1.3727\n",
      "Epoch: 33/110... Step: 4530... Loss: 1.3378... Val Loss: 1.3779\n",
      "Epoch: 33/110... Step: 4540... Loss: 1.3657... Val Loss: 1.3750\n",
      "Epoch: 33/110... Step: 4550... Loss: 1.3927... Val Loss: 1.3773\n",
      "Epoch: 33/110... Step: 4560... Loss: 1.3486... Val Loss: 1.3773\n",
      "Epoch: 33/110... Step: 4570... Loss: 1.3496... Val Loss: 1.3709\n",
      "Epoch: 33/110... Step: 4580... Loss: 1.3869... Val Loss: 1.3713\n",
      "Epoch: 34/110... Step: 4590... Loss: 1.3789... Val Loss: 1.3751\n",
      "Epoch: 34/110... Step: 4600... Loss: 1.3577... Val Loss: 1.3722\n",
      "Epoch: 34/110... Step: 4610... Loss: 1.3841... Val Loss: 1.3670\n",
      "Epoch: 34/110... Step: 4620... Loss: 1.3338... Val Loss: 1.3666\n",
      "Epoch: 34/110... Step: 4630... Loss: 1.3237... Val Loss: 1.3708\n",
      "Epoch: 34/110... Step: 4640... Loss: 1.3772... Val Loss: 1.3685\n",
      "Epoch: 34/110... Step: 4650... Loss: 1.3806... Val Loss: 1.3689\n",
      "Epoch: 34/110... Step: 4660... Loss: 1.3712... Val Loss: 1.3653\n",
      "Epoch: 34/110... Step: 4670... Loss: 1.3862... Val Loss: 1.3695\n",
      "Epoch: 34/110... Step: 4680... Loss: 1.3755... Val Loss: 1.3676\n",
      "Epoch: 34/110... Step: 4690... Loss: 1.3773... Val Loss: 1.3698\n",
      "Epoch: 34/110... Step: 4700... Loss: 1.3733... Val Loss: 1.3715\n",
      "Epoch: 34/110... Step: 4710... Loss: 1.3295... Val Loss: 1.3678\n",
      "Epoch: 34/110... Step: 4720... Loss: 1.3887... Val Loss: 1.3618\n",
      "Epoch: 35/110... Step: 4730... Loss: 1.3642... Val Loss: 1.3728\n",
      "Epoch: 35/110... Step: 4740... Loss: 1.3685... Val Loss: 1.3683\n",
      "Epoch: 35/110... Step: 4750... Loss: 1.3591... Val Loss: 1.3687\n",
      "Epoch: 35/110... Step: 4760... Loss: 1.3540... Val Loss: 1.3640\n",
      "Epoch: 35/110... Step: 4770... Loss: 1.3539... Val Loss: 1.3637\n",
      "Epoch: 35/110... Step: 4780... Loss: 1.3375... Val Loss: 1.3697\n",
      "Epoch: 35/110... Step: 4790... Loss: 1.3586... Val Loss: 1.3666\n",
      "Epoch: 35/110... Step: 4800... Loss: 1.3731... Val Loss: 1.3665\n",
      "Epoch: 35/110... Step: 4810... Loss: 1.3471... Val Loss: 1.3687\n",
      "Epoch: 35/110... Step: 4820... Loss: 1.3673... Val Loss: 1.3646\n",
      "Epoch: 35/110... Step: 4830... Loss: 1.3507... Val Loss: 1.3646\n",
      "Epoch: 35/110... Step: 4840... Loss: 1.3667... Val Loss: 1.3643\n",
      "Epoch: 35/110... Step: 4850... Loss: 1.3632... Val Loss: 1.3672\n",
      "Epoch: 35/110... Step: 4860... Loss: 1.3627... Val Loss: 1.3635\n",
      "Epoch: 36/110... Step: 4870... Loss: 1.3724... Val Loss: 1.3719\n",
      "Epoch: 36/110... Step: 4880... Loss: 1.3516... Val Loss: 1.3630\n",
      "Epoch: 36/110... Step: 4890... Loss: 1.3467... Val Loss: 1.3618\n",
      "Epoch: 36/110... Step: 4900... Loss: 1.3571... Val Loss: 1.3612\n",
      "Epoch: 36/110... Step: 4910... Loss: 1.3385... Val Loss: 1.3612\n",
      "Epoch: 36/110... Step: 4920... Loss: 1.3413... Val Loss: 1.3631\n",
      "Epoch: 36/110... Step: 4930... Loss: 1.3706... Val Loss: 1.3584\n",
      "Epoch: 36/110... Step: 4940... Loss: 1.3496... Val Loss: 1.3575\n",
      "Epoch: 36/110... Step: 4950... Loss: 1.3566... Val Loss: 1.3647\n",
      "Epoch: 36/110... Step: 4960... Loss: 1.3463... Val Loss: 1.3566\n",
      "Epoch: 36/110... Step: 4970... Loss: 1.3610... Val Loss: 1.3572\n",
      "Epoch: 36/110... Step: 4980... Loss: 1.3494... Val Loss: 1.3610\n",
      "Epoch: 36/110... Step: 4990... Loss: 1.3053... Val Loss: 1.3552\n",
      "Epoch: 36/110... Step: 5000... Loss: 1.3628... Val Loss: 1.3589\n",
      "Epoch: 37/110... Step: 5010... Loss: 1.3355... Val Loss: 1.3671\n",
      "Epoch: 37/110... Step: 5020... Loss: 1.3480... Val Loss: 1.3594\n",
      "Epoch: 37/110... Step: 5030... Loss: 1.3294... Val Loss: 1.3593\n",
      "Epoch: 37/110... Step: 5040... Loss: 1.3413... Val Loss: 1.3594\n",
      "Epoch: 37/110... Step: 5050... Loss: 1.3507... Val Loss: 1.3579\n",
      "Epoch: 37/110... Step: 5060... Loss: 1.3544... Val Loss: 1.3595\n",
      "Epoch: 37/110... Step: 5070... Loss: 1.3496... Val Loss: 1.3582\n",
      "Epoch: 37/110... Step: 5080... Loss: 1.3173... Val Loss: 1.3561\n",
      "Epoch: 37/110... Step: 5090... Loss: 1.3435... Val Loss: 1.3591\n",
      "Epoch: 37/110... Step: 5100... Loss: 1.3410... Val Loss: 1.3542\n",
      "Epoch: 37/110... Step: 5110... Loss: 1.3384... Val Loss: 1.3542\n",
      "Epoch: 37/110... Step: 5120... Loss: 1.3613... Val Loss: 1.3565\n",
      "Epoch: 37/110... Step: 5130... Loss: 1.3641... Val Loss: 1.3529\n",
      "Epoch: 37/110... Step: 5140... Loss: 1.3660... Val Loss: 1.3537\n",
      "Epoch: 38/110... Step: 5150... Loss: 1.3302... Val Loss: 1.3608\n",
      "Epoch: 38/110... Step: 5160... Loss: 1.3482... Val Loss: 1.3547\n",
      "Epoch: 38/110... Step: 5170... Loss: 1.3340... Val Loss: 1.3556\n",
      "Epoch: 38/110... Step: 5180... Loss: 1.3785... Val Loss: 1.3532\n",
      "Epoch: 38/110... Step: 5190... Loss: 1.3692... Val Loss: 1.3541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38/110... Step: 5200... Loss: 1.3446... Val Loss: 1.3568\n",
      "Epoch: 38/110... Step: 5210... Loss: 1.3576... Val Loss: 1.3546\n",
      "Epoch: 38/110... Step: 5220... Loss: 1.3404... Val Loss: 1.3533\n",
      "Epoch: 38/110... Step: 5230... Loss: 1.3328... Val Loss: 1.3547\n",
      "Epoch: 38/110... Step: 5240... Loss: 1.3423... Val Loss: 1.3527\n",
      "Epoch: 38/110... Step: 5250... Loss: 1.3359... Val Loss: 1.3523\n",
      "Epoch: 38/110... Step: 5260... Loss: 1.3347... Val Loss: 1.3540\n",
      "Epoch: 38/110... Step: 5270... Loss: 1.3266... Val Loss: 1.3531\n",
      "Epoch: 38/110... Step: 5280... Loss: 1.3286... Val Loss: 1.3527\n",
      "Epoch: 39/110... Step: 5290... Loss: 1.3358... Val Loss: 1.3571\n",
      "Epoch: 39/110... Step: 5300... Loss: 1.3545... Val Loss: 1.3534\n",
      "Epoch: 39/110... Step: 5310... Loss: 1.3592... Val Loss: 1.3509\n",
      "Epoch: 39/110... Step: 5320... Loss: 1.3699... Val Loss: 1.3490\n",
      "Epoch: 39/110... Step: 5330... Loss: 1.3317... Val Loss: 1.3494\n",
      "Epoch: 39/110... Step: 5340... Loss: 1.3439... Val Loss: 1.3576\n",
      "Epoch: 39/110... Step: 5350... Loss: 1.3210... Val Loss: 1.3526\n",
      "Epoch: 39/110... Step: 5360... Loss: 1.3702... Val Loss: 1.3506\n",
      "Epoch: 39/110... Step: 5370... Loss: 1.3226... Val Loss: 1.3501\n",
      "Epoch: 39/110... Step: 5380... Loss: 1.3239... Val Loss: 1.3520\n",
      "Epoch: 39/110... Step: 5390... Loss: 1.3355... Val Loss: 1.3506\n",
      "Epoch: 39/110... Step: 5400... Loss: 1.3117... Val Loss: 1.3543\n",
      "Epoch: 39/110... Step: 5410... Loss: 1.3313... Val Loss: 1.3482\n",
      "Epoch: 39/110... Step: 5420... Loss: 1.3456... Val Loss: 1.3531\n",
      "Epoch: 40/110... Step: 5430... Loss: 1.3496... Val Loss: 1.3556\n",
      "Epoch: 40/110... Step: 5440... Loss: 1.3449... Val Loss: 1.3472\n",
      "Epoch: 40/110... Step: 5450... Loss: 1.3591... Val Loss: 1.3484\n",
      "Epoch: 40/110... Step: 5460... Loss: 1.3490... Val Loss: 1.3524\n",
      "Epoch: 40/110... Step: 5470... Loss: 1.3315... Val Loss: 1.3505\n",
      "Epoch: 40/110... Step: 5480... Loss: 1.3420... Val Loss: 1.3512\n",
      "Epoch: 40/110... Step: 5490... Loss: 1.3254... Val Loss: 1.3511\n",
      "Epoch: 40/110... Step: 5500... Loss: 1.3225... Val Loss: 1.3523\n",
      "Epoch: 40/110... Step: 5510... Loss: 1.3066... Val Loss: 1.3506\n",
      "Epoch: 40/110... Step: 5520... Loss: 1.3203... Val Loss: 1.3509\n",
      "Epoch: 40/110... Step: 5530... Loss: 1.3182... Val Loss: 1.3505\n",
      "Epoch: 40/110... Step: 5540... Loss: 1.3153... Val Loss: 1.3534\n",
      "Epoch: 40/110... Step: 5550... Loss: 1.3512... Val Loss: 1.3489\n",
      "Epoch: 40/110... Step: 5560... Loss: 1.3744... Val Loss: 1.3477\n",
      "Epoch: 41/110... Step: 5570... Loss: 1.3549... Val Loss: 1.3518\n",
      "Epoch: 41/110... Step: 5580... Loss: 1.3793... Val Loss: 1.3473\n",
      "Epoch: 41/110... Step: 5590... Loss: 1.3743... Val Loss: 1.3426\n",
      "Epoch: 41/110... Step: 5600... Loss: 1.3383... Val Loss: 1.3471\n",
      "Epoch: 41/110... Step: 5610... Loss: 1.3581... Val Loss: 1.3476\n",
      "Epoch: 41/110... Step: 5620... Loss: 1.2930... Val Loss: 1.3482\n",
      "Epoch: 41/110... Step: 5630... Loss: 1.3295... Val Loss: 1.3459\n",
      "Epoch: 41/110... Step: 5640... Loss: 1.3123... Val Loss: 1.3478\n",
      "Epoch: 41/110... Step: 5650... Loss: 1.3393... Val Loss: 1.3485\n",
      "Epoch: 41/110... Step: 5660... Loss: 1.3290... Val Loss: 1.3495\n",
      "Epoch: 41/110... Step: 5670... Loss: 1.3141... Val Loss: 1.3460\n",
      "Epoch: 41/110... Step: 5680... Loss: 1.3055... Val Loss: 1.3527\n",
      "Epoch: 41/110... Step: 5690... Loss: 1.3290... Val Loss: 1.3466\n",
      "Epoch: 42/110... Step: 5700... Loss: 1.3918... Val Loss: 1.3440\n",
      "Epoch: 42/110... Step: 5710... Loss: 1.3473... Val Loss: 1.3486\n",
      "Epoch: 42/110... Step: 5720... Loss: 1.3513... Val Loss: 1.3452\n",
      "Epoch: 42/110... Step: 5730... Loss: 1.3648... Val Loss: 1.3407\n",
      "Epoch: 42/110... Step: 5740... Loss: 1.3168... Val Loss: 1.3465\n",
      "Epoch: 42/110... Step: 5750... Loss: 1.2982... Val Loss: 1.3466\n",
      "Epoch: 42/110... Step: 5760... Loss: 1.2985... Val Loss: 1.3468\n",
      "Epoch: 42/110... Step: 5770... Loss: 1.3170... Val Loss: 1.3450\n",
      "Epoch: 42/110... Step: 5780... Loss: 1.3132... Val Loss: 1.3463\n",
      "Epoch: 42/110... Step: 5790... Loss: 1.3101... Val Loss: 1.3443\n",
      "Epoch: 42/110... Step: 5800... Loss: 1.3473... Val Loss: 1.3456\n",
      "Epoch: 42/110... Step: 5810... Loss: 1.3055... Val Loss: 1.3450\n",
      "Epoch: 42/110... Step: 5820... Loss: 1.2948... Val Loss: 1.3469\n",
      "Epoch: 42/110... Step: 5830... Loss: 1.3531... Val Loss: 1.3435\n",
      "Epoch: 43/110... Step: 5840... Loss: 1.3209... Val Loss: 1.3471\n",
      "Epoch: 43/110... Step: 5850... Loss: 1.3389... Val Loss: 1.3463\n",
      "Epoch: 43/110... Step: 5860... Loss: 1.3132... Val Loss: 1.3426\n",
      "Epoch: 43/110... Step: 5870... Loss: 1.3166... Val Loss: 1.3373\n",
      "Epoch: 43/110... Step: 5880... Loss: 1.2986... Val Loss: 1.3425\n",
      "Epoch: 43/110... Step: 5890... Loss: 1.3116... Val Loss: 1.3433\n",
      "Epoch: 43/110... Step: 5900... Loss: 1.3476... Val Loss: 1.3412\n",
      "Epoch: 43/110... Step: 5910... Loss: 1.3218... Val Loss: 1.3447\n",
      "Epoch: 43/110... Step: 5920... Loss: 1.2830... Val Loss: 1.3451\n",
      "Epoch: 43/110... Step: 5930... Loss: 1.3155... Val Loss: 1.3411\n",
      "Epoch: 43/110... Step: 5940... Loss: 1.3353... Val Loss: 1.3418\n",
      "Epoch: 43/110... Step: 5950... Loss: 1.3122... Val Loss: 1.3420\n",
      "Epoch: 43/110... Step: 5960... Loss: 1.2975... Val Loss: 1.3418\n",
      "Epoch: 43/110... Step: 5970... Loss: 1.3202... Val Loss: 1.3399\n",
      "Epoch: 44/110... Step: 5980... Loss: 1.3283... Val Loss: 1.3423\n",
      "Epoch: 44/110... Step: 5990... Loss: 1.3127... Val Loss: 1.3445\n",
      "Epoch: 44/110... Step: 6000... Loss: 1.3356... Val Loss: 1.3414\n",
      "Epoch: 44/110... Step: 6010... Loss: 1.2793... Val Loss: 1.3367\n",
      "Epoch: 44/110... Step: 6020... Loss: 1.2723... Val Loss: 1.3421\n",
      "Epoch: 44/110... Step: 6030... Loss: 1.3246... Val Loss: 1.3403\n",
      "Epoch: 44/110... Step: 6040... Loss: 1.3366... Val Loss: 1.3370\n",
      "Epoch: 44/110... Step: 6050... Loss: 1.3281... Val Loss: 1.3383\n",
      "Epoch: 44/110... Step: 6060... Loss: 1.3377... Val Loss: 1.3397\n",
      "Epoch: 44/110... Step: 6070... Loss: 1.3304... Val Loss: 1.3392\n",
      "Epoch: 44/110... Step: 6080... Loss: 1.3294... Val Loss: 1.3402\n",
      "Epoch: 44/110... Step: 6090... Loss: 1.3305... Val Loss: 1.3392\n",
      "Epoch: 44/110... Step: 6100... Loss: 1.2851... Val Loss: 1.3391\n",
      "Epoch: 44/110... Step: 6110... Loss: 1.3442... Val Loss: 1.3391\n",
      "Epoch: 45/110... Step: 6120... Loss: 1.3119... Val Loss: 1.3463\n",
      "Epoch: 45/110... Step: 6130... Loss: 1.3175... Val Loss: 1.3418\n",
      "Epoch: 45/110... Step: 6140... Loss: 1.3153... Val Loss: 1.3377\n",
      "Epoch: 45/110... Step: 6150... Loss: 1.3076... Val Loss: 1.3354\n",
      "Epoch: 45/110... Step: 6160... Loss: 1.3029... Val Loss: 1.3384\n",
      "Epoch: 45/110... Step: 6170... Loss: 1.2880... Val Loss: 1.3398\n",
      "Epoch: 45/110... Step: 6180... Loss: 1.3173... Val Loss: 1.3362\n",
      "Epoch: 45/110... Step: 6190... Loss: 1.3229... Val Loss: 1.3355\n",
      "Epoch: 45/110... Step: 6200... Loss: 1.3006... Val Loss: 1.3393\n",
      "Epoch: 45/110... Step: 6210... Loss: 1.3258... Val Loss: 1.3419\n",
      "Epoch: 45/110... Step: 6220... Loss: 1.2967... Val Loss: 1.3408\n",
      "Epoch: 45/110... Step: 6230... Loss: 1.3183... Val Loss: 1.3370\n",
      "Epoch: 45/110... Step: 6240... Loss: 1.3217... Val Loss: 1.3366\n",
      "Epoch: 45/110... Step: 6250... Loss: 1.3181... Val Loss: 1.3394\n",
      "Epoch: 46/110... Step: 6260... Loss: 1.3332... Val Loss: 1.3388\n",
      "Epoch: 46/110... Step: 6270... Loss: 1.3108... Val Loss: 1.3354\n",
      "Epoch: 46/110... Step: 6280... Loss: 1.2991... Val Loss: 1.3334\n",
      "Epoch: 46/110... Step: 6290... Loss: 1.3116... Val Loss: 1.3361\n",
      "Epoch: 46/110... Step: 6300... Loss: 1.2951... Val Loss: 1.3350\n",
      "Epoch: 46/110... Step: 6310... Loss: 1.3022... Val Loss: 1.3354\n",
      "Epoch: 46/110... Step: 6320... Loss: 1.3294... Val Loss: 1.3326\n",
      "Epoch: 46/110... Step: 6330... Loss: 1.3062... Val Loss: 1.3348\n",
      "Epoch: 46/110... Step: 6340... Loss: 1.3057... Val Loss: 1.3376\n",
      "Epoch: 46/110... Step: 6350... Loss: 1.3018... Val Loss: 1.3343\n",
      "Epoch: 46/110... Step: 6360... Loss: 1.3194... Val Loss: 1.3350\n",
      "Epoch: 46/110... Step: 6370... Loss: 1.3019... Val Loss: 1.3361\n",
      "Epoch: 46/110... Step: 6380... Loss: 1.2758... Val Loss: 1.3325\n",
      "Epoch: 46/110... Step: 6390... Loss: 1.3208... Val Loss: 1.3352\n",
      "Epoch: 47/110... Step: 6400... Loss: 1.2931... Val Loss: 1.3381\n",
      "Epoch: 47/110... Step: 6410... Loss: 1.2979... Val Loss: 1.3348\n",
      "Epoch: 47/110... Step: 6420... Loss: 1.2986... Val Loss: 1.3348\n",
      "Epoch: 47/110... Step: 6430... Loss: 1.3038... Val Loss: 1.3325\n",
      "Epoch: 47/110... Step: 6440... Loss: 1.3081... Val Loss: 1.3345\n",
      "Epoch: 47/110... Step: 6450... Loss: 1.3164... Val Loss: 1.3329\n",
      "Epoch: 47/110... Step: 6460... Loss: 1.3029... Val Loss: 1.3350\n",
      "Epoch: 47/110... Step: 6470... Loss: 1.2827... Val Loss: 1.3365\n",
      "Epoch: 47/110... Step: 6480... Loss: 1.3046... Val Loss: 1.3380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/110... Step: 6490... Loss: 1.2933... Val Loss: 1.3348\n",
      "Epoch: 47/110... Step: 6500... Loss: 1.3027... Val Loss: 1.3344\n",
      "Epoch: 47/110... Step: 6510... Loss: 1.3110... Val Loss: 1.3348\n",
      "Epoch: 47/110... Step: 6520... Loss: 1.3141... Val Loss: 1.3326\n",
      "Epoch: 47/110... Step: 6530... Loss: 1.3182... Val Loss: 1.3329\n",
      "Epoch: 48/110... Step: 6540... Loss: 1.2892... Val Loss: 1.3319\n",
      "Epoch: 48/110... Step: 6550... Loss: 1.3035... Val Loss: 1.3314\n",
      "Epoch: 48/110... Step: 6560... Loss: 1.2995... Val Loss: 1.3349\n",
      "Epoch: 48/110... Step: 6570... Loss: 1.3292... Val Loss: 1.3307\n",
      "Epoch: 48/110... Step: 6580... Loss: 1.3179... Val Loss: 1.3281\n",
      "Epoch: 48/110... Step: 6590... Loss: 1.2941... Val Loss: 1.3330\n",
      "Epoch: 48/110... Step: 6600... Loss: 1.3131... Val Loss: 1.3315\n",
      "Epoch: 48/110... Step: 6610... Loss: 1.3004... Val Loss: 1.3313\n",
      "Epoch: 48/110... Step: 6620... Loss: 1.2931... Val Loss: 1.3331\n",
      "Epoch: 48/110... Step: 6630... Loss: 1.3065... Val Loss: 1.3295\n",
      "Epoch: 48/110... Step: 6640... Loss: 1.3052... Val Loss: 1.3305\n",
      "Epoch: 48/110... Step: 6650... Loss: 1.2926... Val Loss: 1.3330\n",
      "Epoch: 48/110... Step: 6660... Loss: 1.2816... Val Loss: 1.3301\n",
      "Epoch: 48/110... Step: 6670... Loss: 1.2957... Val Loss: 1.3316\n",
      "Epoch: 49/110... Step: 6680... Loss: 1.3080... Val Loss: 1.3314\n",
      "Epoch: 49/110... Step: 6690... Loss: 1.3204... Val Loss: 1.3303\n",
      "Epoch: 49/110... Step: 6700... Loss: 1.3196... Val Loss: 1.3307\n",
      "Epoch: 49/110... Step: 6710... Loss: 1.3229... Val Loss: 1.3277\n",
      "Epoch: 49/110... Step: 6720... Loss: 1.2906... Val Loss: 1.3273\n",
      "Epoch: 49/110... Step: 6730... Loss: 1.3029... Val Loss: 1.3319\n",
      "Epoch: 49/110... Step: 6740... Loss: 1.2840... Val Loss: 1.3325\n",
      "Epoch: 49/110... Step: 6750... Loss: 1.3266... Val Loss: 1.3363\n",
      "Epoch: 49/110... Step: 6760... Loss: 1.2809... Val Loss: 1.3353\n",
      "Epoch: 49/110... Step: 6770... Loss: 1.2805... Val Loss: 1.3310\n",
      "Epoch: 49/110... Step: 6780... Loss: 1.2941... Val Loss: 1.3345\n",
      "Epoch: 49/110... Step: 6790... Loss: 1.2905... Val Loss: 1.3349\n",
      "Epoch: 49/110... Step: 6800... Loss: 1.2827... Val Loss: 1.3321\n",
      "Epoch: 49/110... Step: 6810... Loss: 1.2985... Val Loss: 1.3292\n",
      "Epoch: 50/110... Step: 6820... Loss: 1.3098... Val Loss: 1.3340\n",
      "Epoch: 50/110... Step: 6830... Loss: 1.3110... Val Loss: 1.3277\n",
      "Epoch: 50/110... Step: 6840... Loss: 1.3102... Val Loss: 1.3280\n",
      "Epoch: 50/110... Step: 6850... Loss: 1.3148... Val Loss: 1.3260\n",
      "Epoch: 50/110... Step: 6860... Loss: 1.2915... Val Loss: 1.3256\n",
      "Epoch: 50/110... Step: 6870... Loss: 1.3048... Val Loss: 1.3260\n",
      "Epoch: 50/110... Step: 6880... Loss: 1.2749... Val Loss: 1.3271\n",
      "Epoch: 50/110... Step: 6890... Loss: 1.2891... Val Loss: 1.3282\n",
      "Epoch: 50/110... Step: 6900... Loss: 1.2676... Val Loss: 1.3303\n",
      "Epoch: 50/110... Step: 6910... Loss: 1.2759... Val Loss: 1.3271\n",
      "Epoch: 50/110... Step: 6920... Loss: 1.2889... Val Loss: 1.3288\n",
      "Epoch: 50/110... Step: 6930... Loss: 1.2737... Val Loss: 1.3341\n",
      "Epoch: 50/110... Step: 6940... Loss: 1.3090... Val Loss: 1.3298\n",
      "Epoch: 50/110... Step: 6950... Loss: 1.3397... Val Loss: 1.3286\n",
      "Epoch: 51/110... Step: 6960... Loss: 1.3283... Val Loss: 1.3310\n",
      "Epoch: 51/110... Step: 6970... Loss: 1.3486... Val Loss: 1.3268\n",
      "Epoch: 51/110... Step: 6980... Loss: 1.3444... Val Loss: 1.3274\n",
      "Epoch: 51/110... Step: 6990... Loss: 1.2987... Val Loss: 1.3246\n",
      "Epoch: 51/110... Step: 7000... Loss: 1.3209... Val Loss: 1.3238\n",
      "Epoch: 51/110... Step: 7010... Loss: 1.2577... Val Loss: 1.3278\n",
      "Epoch: 51/110... Step: 7020... Loss: 1.2947... Val Loss: 1.3269\n",
      "Epoch: 51/110... Step: 7030... Loss: 1.2829... Val Loss: 1.3260\n",
      "Epoch: 51/110... Step: 7040... Loss: 1.3036... Val Loss: 1.3265\n",
      "Epoch: 51/110... Step: 7050... Loss: 1.2854... Val Loss: 1.3236\n",
      "Epoch: 51/110... Step: 7060... Loss: 1.2780... Val Loss: 1.3288\n",
      "Epoch: 51/110... Step: 7070... Loss: 1.2652... Val Loss: 1.3315\n",
      "Epoch: 51/110... Step: 7080... Loss: 1.2916... Val Loss: 1.3273\n",
      "Epoch: 52/110... Step: 7090... Loss: 1.3625... Val Loss: 1.3252\n",
      "Epoch: 52/110... Step: 7100... Loss: 1.3151... Val Loss: 1.3266\n",
      "Epoch: 52/110... Step: 7110... Loss: 1.3111... Val Loss: 1.3226\n",
      "Epoch: 52/110... Step: 7120... Loss: 1.3212... Val Loss: 1.3239\n",
      "Epoch: 52/110... Step: 7130... Loss: 1.2772... Val Loss: 1.3238\n",
      "Epoch: 52/110... Step: 7140... Loss: 1.2673... Val Loss: 1.3259\n",
      "Epoch: 52/110... Step: 7150... Loss: 1.2632... Val Loss: 1.3273\n",
      "Epoch: 52/110... Step: 7160... Loss: 1.2807... Val Loss: 1.3285\n",
      "Epoch: 52/110... Step: 7170... Loss: 1.2920... Val Loss: 1.3290\n",
      "Epoch: 52/110... Step: 7180... Loss: 1.2794... Val Loss: 1.3284\n",
      "Epoch: 52/110... Step: 7190... Loss: 1.3036... Val Loss: 1.3258\n",
      "Epoch: 52/110... Step: 7200... Loss: 1.2768... Val Loss: 1.3255\n",
      "Epoch: 52/110... Step: 7210... Loss: 1.2592... Val Loss: 1.3295\n",
      "Epoch: 52/110... Step: 7220... Loss: 1.3179... Val Loss: 1.3215\n",
      "Epoch: 53/110... Step: 7230... Loss: 1.2890... Val Loss: 1.3239\n",
      "Epoch: 53/110... Step: 7240... Loss: 1.2962... Val Loss: 1.3269\n",
      "Epoch: 53/110... Step: 7250... Loss: 1.2844... Val Loss: 1.3231\n",
      "Epoch: 53/110... Step: 7260... Loss: 1.2821... Val Loss: 1.3218\n",
      "Epoch: 53/110... Step: 7270... Loss: 1.2739... Val Loss: 1.3227\n",
      "Epoch: 53/110... Step: 7280... Loss: 1.2742... Val Loss: 1.3259\n",
      "Epoch: 53/110... Step: 7290... Loss: 1.3172... Val Loss: 1.3251\n",
      "Epoch: 53/110... Step: 7300... Loss: 1.2754... Val Loss: 1.3235\n",
      "Epoch: 53/110... Step: 7310... Loss: 1.2486... Val Loss: 1.3278\n",
      "Epoch: 53/110... Step: 7320... Loss: 1.2839... Val Loss: 1.3239\n",
      "Epoch: 53/110... Step: 7330... Loss: 1.3007... Val Loss: 1.3259\n",
      "Epoch: 53/110... Step: 7340... Loss: 1.2841... Val Loss: 1.3247\n",
      "Epoch: 53/110... Step: 7350... Loss: 1.2749... Val Loss: 1.3264\n",
      "Epoch: 53/110... Step: 7360... Loss: 1.2944... Val Loss: 1.3232\n",
      "Epoch: 54/110... Step: 7370... Loss: 1.2944... Val Loss: 1.3246\n",
      "Epoch: 54/110... Step: 7380... Loss: 1.2842... Val Loss: 1.3355\n",
      "Epoch: 54/110... Step: 7390... Loss: 1.3179... Val Loss: 1.3282\n",
      "Epoch: 54/110... Step: 7400... Loss: 1.2508... Val Loss: 1.3228\n",
      "Epoch: 54/110... Step: 7410... Loss: 1.2393... Val Loss: 1.3225\n",
      "Epoch: 54/110... Step: 7420... Loss: 1.3023... Val Loss: 1.3293\n",
      "Epoch: 54/110... Step: 7430... Loss: 1.3001... Val Loss: 1.3269\n",
      "Epoch: 54/110... Step: 7440... Loss: 1.2984... Val Loss: 1.3247\n",
      "Epoch: 54/110... Step: 7450... Loss: 1.3034... Val Loss: 1.3246\n",
      "Epoch: 54/110... Step: 7460... Loss: 1.2903... Val Loss: 1.3259\n",
      "Epoch: 54/110... Step: 7470... Loss: 1.2929... Val Loss: 1.3242\n",
      "Epoch: 54/110... Step: 7480... Loss: 1.2911... Val Loss: 1.3258\n",
      "Epoch: 54/110... Step: 7490... Loss: 1.2582... Val Loss: 1.3276\n",
      "Epoch: 54/110... Step: 7500... Loss: 1.3164... Val Loss: 1.3211\n",
      "Epoch: 55/110... Step: 7510... Loss: 1.2840... Val Loss: 1.3232\n",
      "Epoch: 55/110... Step: 7520... Loss: 1.2862... Val Loss: 1.3250\n",
      "Epoch: 55/110... Step: 7530... Loss: 1.2842... Val Loss: 1.3218\n",
      "Epoch: 55/110... Step: 7540... Loss: 1.2745... Val Loss: 1.3211\n",
      "Epoch: 55/110... Step: 7550... Loss: 1.2762... Val Loss: 1.3221\n",
      "Epoch: 55/110... Step: 7560... Loss: 1.2634... Val Loss: 1.3261\n",
      "Epoch: 55/110... Step: 7570... Loss: 1.2829... Val Loss: 1.3222\n",
      "Epoch: 55/110... Step: 7580... Loss: 1.2967... Val Loss: 1.3218\n",
      "Epoch: 55/110... Step: 7590... Loss: 1.2728... Val Loss: 1.3257\n",
      "Epoch: 55/110... Step: 7600... Loss: 1.2891... Val Loss: 1.3229\n",
      "Epoch: 55/110... Step: 7610... Loss: 1.2789... Val Loss: 1.3206\n",
      "Epoch: 55/110... Step: 7620... Loss: 1.2851... Val Loss: 1.3213\n",
      "Epoch: 55/110... Step: 7630... Loss: 1.2988... Val Loss: 1.3207\n",
      "Epoch: 55/110... Step: 7640... Loss: 1.2821... Val Loss: 1.3190\n",
      "Epoch: 56/110... Step: 7650... Loss: 1.2918... Val Loss: 1.3250\n",
      "Epoch: 56/110... Step: 7660... Loss: 1.2803... Val Loss: 1.3224\n",
      "Epoch: 56/110... Step: 7670... Loss: 1.2720... Val Loss: 1.3199\n",
      "Epoch: 56/110... Step: 7680... Loss: 1.2926... Val Loss: 1.3208\n",
      "Epoch: 56/110... Step: 7690... Loss: 1.2704... Val Loss: 1.3241\n",
      "Epoch: 56/110... Step: 7700... Loss: 1.2794... Val Loss: 1.3257\n",
      "Epoch: 56/110... Step: 7710... Loss: 1.2965... Val Loss: 1.3226\n",
      "Epoch: 56/110... Step: 7720... Loss: 1.2817... Val Loss: 1.3211\n",
      "Epoch: 56/110... Step: 7730... Loss: 1.2822... Val Loss: 1.3236\n",
      "Epoch: 56/110... Step: 7740... Loss: 1.2713... Val Loss: 1.3239\n",
      "Epoch: 56/110... Step: 7750... Loss: 1.2963... Val Loss: 1.3192\n",
      "Epoch: 56/110... Step: 7760... Loss: 1.2733... Val Loss: 1.3230\n",
      "Epoch: 56/110... Step: 7770... Loss: 1.2331... Val Loss: 1.3200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 56/110... Step: 7780... Loss: 1.2892... Val Loss: 1.3195\n",
      "Epoch: 57/110... Step: 7790... Loss: 1.2551... Val Loss: 1.3258\n",
      "Epoch: 57/110... Step: 7800... Loss: 1.2661... Val Loss: 1.3243\n",
      "Epoch: 57/110... Step: 7810... Loss: 1.2617... Val Loss: 1.3173\n",
      "Epoch: 57/110... Step: 7820... Loss: 1.2629... Val Loss: 1.3184\n",
      "Epoch: 57/110... Step: 7830... Loss: 1.2872... Val Loss: 1.3210\n",
      "Epoch: 57/110... Step: 7840... Loss: 1.2901... Val Loss: 1.3218\n",
      "Epoch: 57/110... Step: 7850... Loss: 1.2787... Val Loss: 1.3231\n",
      "Epoch: 57/110... Step: 7860... Loss: 1.2454... Val Loss: 1.3200\n",
      "Epoch: 57/110... Step: 7870... Loss: 1.2813... Val Loss: 1.3219\n",
      "Epoch: 57/110... Step: 7880... Loss: 1.2691... Val Loss: 1.3227\n",
      "Epoch: 57/110... Step: 7890... Loss: 1.2689... Val Loss: 1.3216\n",
      "Epoch: 57/110... Step: 7900... Loss: 1.2905... Val Loss: 1.3229\n",
      "Epoch: 57/110... Step: 7910... Loss: 1.2854... Val Loss: 1.3194\n",
      "Epoch: 57/110... Step: 7920... Loss: 1.2930... Val Loss: 1.3235\n",
      "Epoch: 58/110... Step: 7930... Loss: 1.2675... Val Loss: 1.3290\n",
      "Epoch: 58/110... Step: 7940... Loss: 1.2750... Val Loss: 1.3211\n",
      "Epoch: 58/110... Step: 7950... Loss: 1.2752... Val Loss: 1.3222\n",
      "Epoch: 58/110... Step: 7960... Loss: 1.2946... Val Loss: 1.3220\n",
      "Epoch: 58/110... Step: 7970... Loss: 1.2943... Val Loss: 1.3190\n",
      "Epoch: 58/110... Step: 7980... Loss: 1.2773... Val Loss: 1.3183\n",
      "Epoch: 58/110... Step: 7990... Loss: 1.2898... Val Loss: 1.3201\n",
      "Epoch: 58/110... Step: 8000... Loss: 1.2702... Val Loss: 1.3176\n",
      "Epoch: 58/110... Step: 8010... Loss: 1.2673... Val Loss: 1.3232\n",
      "Epoch: 58/110... Step: 8020... Loss: 1.2773... Val Loss: 1.3213\n",
      "Epoch: 58/110... Step: 8030... Loss: 1.2806... Val Loss: 1.3215\n",
      "Epoch: 58/110... Step: 8040... Loss: 1.2623... Val Loss: 1.3182\n",
      "Epoch: 58/110... Step: 8050... Loss: 1.2637... Val Loss: 1.3195\n",
      "Epoch: 58/110... Step: 8060... Loss: 1.2674... Val Loss: 1.3208\n",
      "Epoch: 59/110... Step: 8070... Loss: 1.2705... Val Loss: 1.3218\n",
      "Epoch: 59/110... Step: 8080... Loss: 1.2878... Val Loss: 1.3178\n",
      "Epoch: 59/110... Step: 8090... Loss: 1.2865... Val Loss: 1.3210\n",
      "Epoch: 59/110... Step: 8100... Loss: 1.3051... Val Loss: 1.3202\n",
      "Epoch: 59/110... Step: 8110... Loss: 1.2625... Val Loss: 1.3172\n",
      "Epoch: 59/110... Step: 8120... Loss: 1.2791... Val Loss: 1.3174\n",
      "Epoch: 59/110... Step: 8130... Loss: 1.2656... Val Loss: 1.3151\n",
      "Epoch: 59/110... Step: 8140... Loss: 1.2997... Val Loss: 1.3166\n",
      "Epoch: 59/110... Step: 8150... Loss: 1.2607... Val Loss: 1.3203\n",
      "Epoch: 59/110... Step: 8160... Loss: 1.2671... Val Loss: 1.3175\n",
      "Epoch: 59/110... Step: 8170... Loss: 1.2700... Val Loss: 1.3184\n",
      "Epoch: 59/110... Step: 8180... Loss: 1.2701... Val Loss: 1.3180\n",
      "Epoch: 59/110... Step: 8190... Loss: 1.2621... Val Loss: 1.3146\n",
      "Epoch: 59/110... Step: 8200... Loss: 1.2644... Val Loss: 1.3176\n",
      "Epoch: 60/110... Step: 8210... Loss: 1.2792... Val Loss: 1.3208\n",
      "Epoch: 60/110... Step: 8220... Loss: 1.2764... Val Loss: 1.3140\n",
      "Epoch: 60/110... Step: 8230... Loss: 1.2946... Val Loss: 1.3138\n",
      "Epoch: 60/110... Step: 8240... Loss: 1.2852... Val Loss: 1.3134\n",
      "Epoch: 60/110... Step: 8250... Loss: 1.2702... Val Loss: 1.3163\n",
      "Epoch: 60/110... Step: 8260... Loss: 1.2885... Val Loss: 1.3155\n",
      "Epoch: 60/110... Step: 8270... Loss: 1.2604... Val Loss: 1.3158\n",
      "Epoch: 60/110... Step: 8280... Loss: 1.2513... Val Loss: 1.3138\n",
      "Epoch: 60/110... Step: 8290... Loss: 1.2549... Val Loss: 1.3159\n",
      "Epoch: 60/110... Step: 8300... Loss: 1.2555... Val Loss: 1.3139\n",
      "Epoch: 60/110... Step: 8310... Loss: 1.2604... Val Loss: 1.3164\n",
      "Epoch: 60/110... Step: 8320... Loss: 1.2456... Val Loss: 1.3199\n",
      "Epoch: 60/110... Step: 8330... Loss: 1.2867... Val Loss: 1.3148\n",
      "Epoch: 60/110... Step: 8340... Loss: 1.3158... Val Loss: 1.3164\n",
      "Epoch: 61/110... Step: 8350... Loss: 1.2973... Val Loss: 1.3240\n",
      "Epoch: 61/110... Step: 8360... Loss: 1.3092... Val Loss: 1.3151\n",
      "Epoch: 61/110... Step: 8370... Loss: 1.3029... Val Loss: 1.3141\n",
      "Epoch: 61/110... Step: 8380... Loss: 1.2835... Val Loss: 1.3119\n",
      "Epoch: 61/110... Step: 8390... Loss: 1.2892... Val Loss: 1.3139\n",
      "Epoch: 61/110... Step: 8400... Loss: 1.2421... Val Loss: 1.3146\n",
      "Epoch: 61/110... Step: 8410... Loss: 1.2721... Val Loss: 1.3158\n",
      "Epoch: 61/110... Step: 8420... Loss: 1.2558... Val Loss: 1.3177\n",
      "Epoch: 61/110... Step: 8430... Loss: 1.2837... Val Loss: 1.3174\n",
      "Epoch: 61/110... Step: 8440... Loss: 1.2659... Val Loss: 1.3150\n",
      "Epoch: 61/110... Step: 8450... Loss: 1.2496... Val Loss: 1.3193\n",
      "Epoch: 61/110... Step: 8460... Loss: 1.2445... Val Loss: 1.3217\n",
      "Epoch: 61/110... Step: 8470... Loss: 1.2729... Val Loss: 1.3160\n",
      "Epoch: 62/110... Step: 8480... Loss: 1.3338... Val Loss: 1.3165\n",
      "Epoch: 62/110... Step: 8490... Loss: 1.2837... Val Loss: 1.3181\n",
      "Epoch: 62/110... Step: 8500... Loss: 1.2791... Val Loss: 1.3145\n",
      "Epoch: 62/110... Step: 8510... Loss: 1.3023... Val Loss: 1.3102\n",
      "Epoch: 62/110... Step: 8520... Loss: 1.2526... Val Loss: 1.3113\n",
      "Epoch: 62/110... Step: 8530... Loss: 1.2444... Val Loss: 1.3130\n",
      "Epoch: 62/110... Step: 8540... Loss: 1.2360... Val Loss: 1.3147\n",
      "Epoch: 62/110... Step: 8550... Loss: 1.2574... Val Loss: 1.3127\n",
      "Epoch: 62/110... Step: 8560... Loss: 1.2539... Val Loss: 1.3143\n",
      "Epoch: 62/110... Step: 8570... Loss: 1.2433... Val Loss: 1.3138\n",
      "Epoch: 62/110... Step: 8580... Loss: 1.2796... Val Loss: 1.3102\n",
      "Epoch: 62/110... Step: 8590... Loss: 1.2466... Val Loss: 1.3160\n",
      "Epoch: 62/110... Step: 8600... Loss: 1.2379... Val Loss: 1.3202\n",
      "Epoch: 62/110... Step: 8610... Loss: 1.2926... Val Loss: 1.3150\n",
      "Epoch: 63/110... Step: 8620... Loss: 1.2593... Val Loss: 1.3150\n",
      "Epoch: 63/110... Step: 8630... Loss: 1.2688... Val Loss: 1.3135\n",
      "Epoch: 63/110... Step: 8640... Loss: 1.2589... Val Loss: 1.3099\n",
      "Epoch: 63/110... Step: 8650... Loss: 1.2611... Val Loss: 1.3096\n",
      "Epoch: 63/110... Step: 8660... Loss: 1.2477... Val Loss: 1.3094\n",
      "Epoch: 63/110... Step: 8670... Loss: 1.2525... Val Loss: 1.3139\n",
      "Epoch: 63/110... Step: 8680... Loss: 1.2827... Val Loss: 1.3143\n",
      "Epoch: 63/110... Step: 8690... Loss: 1.2550... Val Loss: 1.3149\n",
      "Epoch: 63/110... Step: 8700... Loss: 1.2327... Val Loss: 1.3133\n",
      "Epoch: 63/110... Step: 8710... Loss: 1.2557... Val Loss: 1.3133\n",
      "Epoch: 63/110... Step: 8720... Loss: 1.2769... Val Loss: 1.3107\n",
      "Epoch: 63/110... Step: 8730... Loss: 1.2521... Val Loss: 1.3142\n",
      "Epoch: 63/110... Step: 8740... Loss: 1.2447... Val Loss: 1.3175\n",
      "Epoch: 63/110... Step: 8750... Loss: 1.2660... Val Loss: 1.3134\n",
      "Epoch: 64/110... Step: 8760... Loss: 1.2692... Val Loss: 1.3176\n",
      "Epoch: 64/110... Step: 8770... Loss: 1.2597... Val Loss: 1.3188\n",
      "Epoch: 64/110... Step: 8780... Loss: 1.2871... Val Loss: 1.3120\n",
      "Epoch: 64/110... Step: 8790... Loss: 1.2298... Val Loss: 1.3106\n",
      "Epoch: 64/110... Step: 8800... Loss: 1.2230... Val Loss: 1.3100\n",
      "Epoch: 64/110... Step: 8810... Loss: 1.2662... Val Loss: 1.3145\n",
      "Epoch: 64/110... Step: 8820... Loss: 1.2823... Val Loss: 1.3149\n",
      "Epoch: 64/110... Step: 8830... Loss: 1.2725... Val Loss: 1.3125\n",
      "Epoch: 64/110... Step: 8840... Loss: 1.2907... Val Loss: 1.3142\n",
      "Epoch: 64/110... Step: 8850... Loss: 1.2738... Val Loss: 1.3147\n",
      "Epoch: 64/110... Step: 8860... Loss: 1.2677... Val Loss: 1.3105\n",
      "Epoch: 64/110... Step: 8870... Loss: 1.2679... Val Loss: 1.3128\n",
      "Epoch: 64/110... Step: 8880... Loss: 1.2338... Val Loss: 1.3149\n",
      "Epoch: 64/110... Step: 8890... Loss: 1.2839... Val Loss: 1.3111\n",
      "Epoch: 65/110... Step: 8900... Loss: 1.2642... Val Loss: 1.3120\n",
      "Epoch: 65/110... Step: 8910... Loss: 1.2649... Val Loss: 1.3122\n",
      "Epoch: 65/110... Step: 8920... Loss: 1.2604... Val Loss: 1.3083\n",
      "Epoch: 65/110... Step: 8930... Loss: 1.2511... Val Loss: 1.3098\n",
      "Epoch: 65/110... Step: 8940... Loss: 1.2513... Val Loss: 1.3098\n",
      "Epoch: 65/110... Step: 8950... Loss: 1.2472... Val Loss: 1.3176\n",
      "Epoch: 65/110... Step: 8960... Loss: 1.2543... Val Loss: 1.3134\n",
      "Epoch: 65/110... Step: 8970... Loss: 1.2692... Val Loss: 1.3136\n",
      "Epoch: 65/110... Step: 8980... Loss: 1.2533... Val Loss: 1.3167\n",
      "Epoch: 65/110... Step: 8990... Loss: 1.2686... Val Loss: 1.3132\n",
      "Epoch: 65/110... Step: 9000... Loss: 1.2542... Val Loss: 1.3100\n",
      "Epoch: 65/110... Step: 9010... Loss: 1.2570... Val Loss: 1.3124\n",
      "Epoch: 65/110... Step: 9020... Loss: 1.2600... Val Loss: 1.3107\n",
      "Epoch: 65/110... Step: 9030... Loss: 1.2634... Val Loss: 1.3126\n",
      "Epoch: 66/110... Step: 9040... Loss: 1.2781... Val Loss: 1.3145\n",
      "Epoch: 66/110... Step: 9050... Loss: 1.2468... Val Loss: 1.3123\n",
      "Epoch: 66/110... Step: 9060... Loss: 1.2418... Val Loss: 1.3085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 66/110... Step: 9070... Loss: 1.2604... Val Loss: 1.3135\n",
      "Epoch: 66/110... Step: 9080... Loss: 1.2443... Val Loss: 1.3126\n",
      "Epoch: 66/110... Step: 9090... Loss: 1.2509... Val Loss: 1.3175\n",
      "Epoch: 66/110... Step: 9100... Loss: 1.2672... Val Loss: 1.3160\n",
      "Epoch: 66/110... Step: 9110... Loss: 1.2567... Val Loss: 1.3156\n",
      "Epoch: 66/110... Step: 9120... Loss: 1.2526... Val Loss: 1.3158\n",
      "Epoch: 66/110... Step: 9130... Loss: 1.2596... Val Loss: 1.3121\n",
      "Epoch: 66/110... Step: 9140... Loss: 1.2648... Val Loss: 1.3087\n",
      "Epoch: 66/110... Step: 9150... Loss: 1.2473... Val Loss: 1.3112\n",
      "Epoch: 66/110... Step: 9160... Loss: 1.2215... Val Loss: 1.3115\n",
      "Epoch: 66/110... Step: 9170... Loss: 1.2665... Val Loss: 1.3115\n",
      "Epoch: 67/110... Step: 9180... Loss: 1.2464... Val Loss: 1.3163\n",
      "Epoch: 67/110... Step: 9190... Loss: 1.2548... Val Loss: 1.3140\n",
      "Epoch: 67/110... Step: 9200... Loss: 1.2469... Val Loss: 1.3109\n",
      "Epoch: 67/110... Step: 9210... Loss: 1.2450... Val Loss: 1.3107\n",
      "Epoch: 67/110... Step: 9220... Loss: 1.2645... Val Loss: 1.3119\n",
      "Epoch: 67/110... Step: 9230... Loss: 1.2655... Val Loss: 1.3168\n",
      "Epoch: 67/110... Step: 9240... Loss: 1.2569... Val Loss: 1.3136\n",
      "Epoch: 67/110... Step: 9250... Loss: 1.2292... Val Loss: 1.3133\n",
      "Epoch: 67/110... Step: 9260... Loss: 1.2576... Val Loss: 1.3156\n",
      "Epoch: 67/110... Step: 9270... Loss: 1.2542... Val Loss: 1.3125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1d1a92243b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-6c7a0384f4a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/furb_deeplearning/.direnv/python-3.9.1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-2b40e0dd0c87>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/furb_deeplearning/.direnv/python-3.9.1/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/furb_deeplearning/.direnv/python-3.9.1/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 110\n",
    "\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d50fc4",
   "metadata": {},
   "source": [
    "### Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c52e6bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T23:09:58.408365Z",
     "start_time": "2021-07-09T23:09:58.397843Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        \n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffc86599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T23:09:59.363302Z",
     "start_time": "2021-07-09T23:09:59.354101Z"
    }
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    net.cpu()\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74e12943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T23:10:11.182089Z",
     "start_time": "2021-07-09T23:10:10.547541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Arkadyevna as he with\n",
      "her hands\n",
      "the carriage and second heart of the same tenderness with shill--which as he thought of him, and standing all he had anything so at home to see the same, who had to branded his side. The muscate\n",
      "son she saw the shooting\n",
      "heart and her\n",
      "person with the corn, and that would be a letting the constitutions with his\n",
      "watch, and sood all his footmines that\n",
      "had any still of any chief true, and altasided the\n",
      "stream and\n",
      "angrily of a mertity and a selencing child, and was suff\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 500, prime='Anna', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
